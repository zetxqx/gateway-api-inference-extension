/*
Copyright The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by applyconfiguration-gen. DO NOT EDIT.

package v1alpha2

// InferenceObjectiveSpecApplyConfiguration represents a declarative configuration of the InferenceObjectiveSpec type for use
// with apply.
//
// InferenceObjectiveSpec represents the desired state of a specific model use case. This resource is
// managed by the "Inference Workload Owner" persona.
//
// The Inference Workload Owner persona is someone that trains, verifies, and
// leverages a large language model from a model frontend, drives the lifecycle
// and rollout of new versions of those models, and defines the specific
// performance and latency goals for the model. These workloads are
// expected to operate within an InferencePool sharing compute capacity with other
// InferenceObjectives, defined by the Inference Platform Admin.
type InferenceObjectiveSpecApplyConfiguration struct {
	// Priority defines how important it is to serve the request compared to other requests in the same pool.
	// Priority is an integer value that defines the priority of the request.
	// The higher the value, the more critical the request is; negative values _are_ allowed.
	// No default value is set for this field, allowing for future additions of new fields that may 'one of' with this field.
	// However, implementations that consume this field (such as the Endpoint Picker) will treat an unset value as '0'.
	// Priority is used in flow control, primarily in the event of resource scarcity(requests need to be queued).
	// All requests will be queued, and flow control will _always_ allow requests of higher priority to be served first.
	// Fairness is only enforced and tracked between requests of the same priority.
	//
	// Example: requests with Priority 10 will always be served before
	// requests with Priority of 0 (the value used if Priority is unset or no InfereneceObjective is specified).
	// Similarly requests with a Priority of -10 will always be served after requests with Priority of 0.
	Priority *int `json:"priority,omitempty"`
	// PoolRef is a reference to the inference pool, the pool must exist in the same namespace.
	PoolRef *PoolObjectReferenceApplyConfiguration `json:"poolRef,omitempty"`
}

// InferenceObjectiveSpecApplyConfiguration constructs a declarative configuration of the InferenceObjectiveSpec type for use with
// apply.
func InferenceObjectiveSpec() *InferenceObjectiveSpecApplyConfiguration {
	return &InferenceObjectiveSpecApplyConfiguration{}
}

// WithPriority sets the Priority field in the declarative configuration to the given value
// and returns the receiver, so that objects can be built by chaining "With" function invocations.
// If called multiple times, the Priority field is set to the value of the last call.
func (b *InferenceObjectiveSpecApplyConfiguration) WithPriority(value int) *InferenceObjectiveSpecApplyConfiguration {
	b.Priority = &value
	return b
}

// WithPoolRef sets the PoolRef field in the declarative configuration to the given value
// and returns the receiver, so that objects can be built by chaining "With" function invocations.
// If called multiple times, the PoolRef field is set to the value of the last call.
func (b *InferenceObjectiveSpecApplyConfiguration) WithPoolRef(value *PoolObjectReferenceApplyConfiguration) *InferenceObjectiveSpecApplyConfiguration {
	b.PoolRef = value
	return b
}
