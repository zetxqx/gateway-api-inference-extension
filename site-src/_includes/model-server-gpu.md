=== "GPU-Based Model Server deployment"

    For this setup, you will need 3 GPUs to run the sample model server. Adjust the number of replicas as needed.
    Create a Hugging Face secret to download the model [Qwen/Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B).
    Ensure that the token grants access to this model.

    Deploy a sample model server deployment with the proper protocol to work with the LLM Instance Gateway.
