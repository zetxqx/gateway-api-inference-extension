{{- define "inference-extension.deployment" -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "gateway-api-inference-extension.name" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gateway-api-inference-extension.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.inferenceExtension.replicas | default 1 }}
  strategy:
    # The current recommended EPP deployment pattern is to have a single active replica. This ensures
    # optimal performance of the stateful operations such prefix cache aware scorer.
    # The Recreate strategy the old replica is killed immediately, and allow the new replica(s) to
    # quickly take over. This is particularly important in the high availability set up with leader
    # election, as the rolling update strategy would prevent the old leader being killed because
    # otherwise the maxUnavailable would be 100%.
    type: Recreate
  selector:
    matchLabels:
      {{- include "gateway-api-inference-extension.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "gateway-api-inference-extension.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "gateway-api-inference-extension.name" . }}
      # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
      terminationGracePeriodSeconds: 130
      containers:
      {{- if .Values.inferenceExtension.sidecar.enabled }}
        - name: {{ .Values.inferenceExtension.sidecar.name }}
          image: {{ .Values.inferenceExtension.sidecar.image }}
          imagePullPolicy: {{ .Values.inferenceExtension.sidecar.imagePullPolicy | default "IfNotPresent" }}
        {{- with .Values.inferenceExtension.sidecar.command }}
          command:
            - {{ . | quote }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.args }}
          args:
          {{- range . }}
            - {{ . | quote }}
          {{- end }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.env }}
          env:
          {{- toYaml . | nindent 10 }}
          {{- end }}
        {{- with .Values.inferenceExtension.sidecar.ports }}
          ports:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.livenessProbe }}
          livenessProbe:
          {{- toYaml . | nindent 12 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.readinessProbe }}
          readinessProbe:
          {{- toYaml . | nindent 12 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.resources }}
          resources:
          {{- toYaml . | nindent 12 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.volumeMounts }}
          volumeMounts:
          {{- toYaml . | nindent 12 }}
        {{- end }}
        {{- end }}
        - name: epp
          image: {{ .Values.inferenceExtension.image.hub }}/{{ .Values.inferenceExtension.image.name }}:{{ .Values.inferenceExtension.image.tag }}
          imagePullPolicy: {{ .Values.inferenceExtension.image.pullPolicy | default "IfNotPresent" }}
          args:
          {{- /* 1. Determine Model Server Type Logic */ -}}
          {{- $modelServerType := "vllm" }}
          {{- if and .Values.inferenceExtension.endpointsServer .Values.inferenceExtension.endpointsServer.standalone -}}
            {{- $modelServerType = .Values.inferenceExtension.endpointsServer.modelServerType | default "vllm" }}
          {{- else }}
            {{- $modelServerType = .Values.inferencePool.modelServerType | default "vllm" }}
          {{- end }}
          {{- /* 2. Mode Specific Flags */ -}}
          {{- if and .Values.inferenceExtension.endpointsServer .Values.inferenceExtension.endpointsServer.standalone }}
              - --endpoint-selector
              - {{ .Values.inferenceExtension.endpointsServer.endpointSelector | quote }}
              - --endpoint-target-ports
              - {{ .Values.inferenceExtension.endpointsServer.targetPorts | quote }}
          {{- else }}
              - --pool-name
              - {{ .Release.Name }}
              # The pool namespace is optional because EPP can default to the NAMESPACE env var.
              - --pool-namespace
              - {{ .Release.Namespace }}
          {{- if ne .Values.inferencePool.apiVersion "inference.networking.k8s.io" }}
              - --pool-group
              - "{{ (split "/" .Values.inferencePool.apiVersion)._0 }}"
          {{- end }}
          {{- end }}
          {{- if eq $modelServerType "triton-tensorrt-llm" }}
              - --total-queued-requests-metric
              - "nv_trt_llm_request_metrics{request_type=waiting}"
              - --kv-cache-usage-percentage-metric
              - "nv_trt_llm_kv_cache_block_metrics{kv_cache_block_type=fraction}"
              - --lora-info-metric
              - "" # Set an empty metric to disable LoRA metric scraping as they are not supported by Triton yet.
          {{- end }}
              - --zap-encoder
              - "json"
              - --config-file
              - "/config/{{ .Values.inferenceExtension.pluginsConfigFile }}"
          {{- if gt (.Values.inferenceExtension.replicas | int) 1 }}
              - --ha-enable-leader-election
          {{- end }}
              # Pass additional flags via the inferenceExtension.flags field in values.yaml.
          {{- range $key, $value := .Values.inferenceExtension.flags }}
              - --{{ $key }}
              - "{{ $value }}"
          {{- end }}
          {{- if .Values.inferenceExtension.tracing.enabled }}
              - --tracing=true
          {{- else }}
              - --tracing=false
          {{- end }}
          {{- if not .Values.inferenceExtension.monitoring.prometheus.auth.enabled }}
              - --metrics-endpoint-auth=false
          {{- end }}
          ports:
            - name: grpc
              containerPort: 9002
            - name: grpc-health
              containerPort: 9003
            - name: metrics
              containerPort: 9090
        {{- if .Values.inferenceExtension.extraContainerPorts }}
        {{- toYaml .Values.inferenceExtension.extraContainerPorts | nindent 8 }}
        {{- end }}
          livenessProbe:
          {{- if gt (.Values.inferenceExtension.replicas | int) 1 }}
            grpc:
              port: 9003
              service: liveness
          {{- else }}
            grpc:
              port: 9003
              service: inference-extension
          {{- end }}
            initialDelaySeconds: 5
            periodSeconds: 10
          readinessProbe:
          {{- if gt (.Values.inferenceExtension.replicas | int) 1 }}
            grpc:
              port: 9003
              service: readiness
          {{- else }}
            grpc:
              port: 9003
              service: inference-extension
          {{- end }}
            periodSeconds: 2
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
        {{- include "gateway-api-inference-extension.latencyPredictor.env" . | nindent 12 }}
        {{- if .Values.inferenceExtension.tracing.enabled }}
            - name: OTEL_SERVICE_NAME
              value: "gateway-api-inference-extension"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: {{ .Values.inferenceExtension.tracing.otelExporterEndpoint | quote }}
            - name: OTEL_TRACES_EXPORTER
              value: "otlp"
            - name: OTEL_RESOURCE_ATTRIBUTES_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: OTEL_RESOURCE_ATTRIBUTES_POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: 'k8s.namespace.name=$(NAMESPACE),k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME)'
            - name: OTEL_TRACES_SAMPLER
              value: {{ .Values.inferenceExtension.tracing.sampling.sampler | quote }}
            - name: OTEL_TRACES_SAMPLER_ARG
              value: {{ .Values.inferenceExtension.tracing.sampling.samplerArg | quote }}
        {{- end }}
        {{- if .Values.inferenceExtension.env }}
        {{- toYaml .Values.inferenceExtension.env | nindent 12 }}
        {{- end }}
          volumeMounts:
            - name: plugins-config-volume
              mountPath: "/config"
        {{- if .Values.inferenceExtension.volumeMounts }}
        {{- toYaml .Values.inferenceExtension.volumeMounts | nindent 12 }}
        {{- end }}
      {{- include "gateway-api-inference-extension.latencyPredictor.containers" . | nindent 8 }}
      volumes:
      {{- if .Values.inferenceExtension.volumes }}
      {{- toYaml .Values.inferenceExtension.volumes | nindent 8 }}
      {{- end }}
      {{- if .Values.inferenceExtension.sidecar.volumes }}
      {{- tpl (toYaml .Values.inferenceExtension.sidecar.volumes) $ | nindent 8 }}
      {{- end }}
        - name: plugins-config-volume
          configMap:
            name: {{ include "gateway-api-inference-extension.name" . }}
      {{- include "gateway-api-inference-extension.latencyPredictor.volumes" . | nindent 8 }}
      {{- if .Values.inferenceExtension.affinity }}
      affinity:
        {{- toYaml .Values.inferenceExtension.affinity | nindent 8 }}
      {{- end }}
      {{- if .Values.inferenceExtension.tolerations }}
      tolerations:
        {{- toYaml .Values.inferenceExtension.tolerations | nindent 8 }}
      {{- end }}
---
{{- end }}
